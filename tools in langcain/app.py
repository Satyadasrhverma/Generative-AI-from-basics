from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace
from langchain.tools import tool
from langchain.agents import AgentExecutor, create_openai_tools_agent
from dotenv import load_dotenv

# Load environment variables (.env)
load_dotenv()

# -----------------------------
# 1Ô∏è‚É£ LLM SETUP (HuggingFace)
# -----------------------------
llm = HuggingFaceEndpoint(
    repo_id="meta-llama/Llama-3.1-8B-Instruct",
    task="text-generation",
)

model = ChatHuggingFace(llm=llm)

# -----------------------------
# 2Ô∏è‚É£ TOOLS
# -----------------------------
@tool
def search(query: str) -> str:
    """Search information based on a query"""
    return f"üîç Search result for: {query}"

@tool
def get_weather(location: str) -> str:
    """Get weather information for a location"""
    return f"üå§ Weather in {location}: Sunny, 30¬∞C"

tools = [search, get_weather]

# -----------------------------
# 3Ô∏è‚É£ SYSTEM PROMPT
# -----------------------------
system_prompt = """
You are a helpful assistant.

Rules:
- If the user asks to SEARCH something, use the search tool.
- If the user asks about WEATHER, use the get_weather tool.
- Do NOT guess information.
- Use tools when required.
"""

# -----------------------------
# 4Ô∏è‚É£ CREATE AGENT (STABLE)
# -----------------------------
agent = create_openai_tools_agent(
    llm=model,
    tools=tools,
    prompt=system_prompt
)

agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True
)

# -----------------------------
# 5Ô∏è‚É£ RUN LOOP
# -----------------------------
while True:
    user_input = input("\nAsk something (search/weather) or 'exit': ")

    if user_input.lower() == "exit":
        print("üëã Exiting...")
        break

    response = agent_executor.invoke(
        {"input": user_input}
    )

    print("\n‚úÖ FINAL ANSWER:")
    print(response["output"])
